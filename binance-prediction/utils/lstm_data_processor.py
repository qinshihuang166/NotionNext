"""
LSTM æ•°æ®å¤„ç†æ¨¡å—
LSTM Data Processing Module

ä¸“é—¨ä¸ºLSTMæ¨¡å‹å¤„ç†æ—¶é—´åºåˆ—æ•°æ®
Specifically processes time-series data for LSTM models

ä½œè€…: qinshihuang166
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from typing import Tuple, Optional, List
import joblib
import os

# å¯¼å…¥é…ç½®
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config_lstm import DataConfig, PathConfig
from utils.technical_indicators import TechnicalIndicators


class LSTMDataProcessor:
    """
    LSTMæ•°æ®å¤„ç†å™¨
    
    ä¸»è¦åŠŸèƒ½:
    1. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†
    2. ç‰¹å¾å·¥ç¨‹ï¼ˆæŠ€æœ¯æŒ‡æ ‡ï¼‰
    3. æ•°æ®å½’ä¸€åŒ–
    4. åˆ›å»ºæ—¶é—´åºåˆ—çª—å£ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
    5. æ•°æ®é›†åˆ’åˆ†ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰
    """
    
    def __init__(self, config: Optional[DataConfig] = None):
        """
        åˆå§‹åŒ–æ•°æ®å¤„ç†å™¨
        
        Args:
            config: æ•°æ®é…ç½®å¯¹è±¡ï¼Œé»˜è®¤ä½¿ç”¨å…¨å±€é…ç½®
        """
        self.config = config or DataConfig()
        self.scaler = None
        self.feature_columns = None
        
    def load_raw_data(self, file_path: Optional[str] = None) -> pd.DataFrame:
        """
        åŠ è½½åŸå§‹æ•°æ®
        
        Args:
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„è·¯å¾„
            
        Returns:
            åŸå§‹æ•°æ®DataFrame
        """
        file_path = file_path or self.config.RAW_DATA_FILE
        
        if not os.path.exists(file_path):
            raise FileNotFoundError(
                f"âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {file_path}\n"
                f"è¯·å…ˆè¿è¡Œ download_data.py ä¸‹è½½æ•°æ®ï¼"
            )
        
        print(f"ğŸ“‚ åŠ è½½æ•°æ®: {file_path}")
        df = pd.read_csv(file_path)
        
        # ç¡®ä¿å¿…éœ€çš„åˆ—å­˜åœ¨
        required_cols = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        missing_cols = [col for col in required_cols if col not in df.columns]
        if missing_cols:
            raise ValueError(f"âŒ æ•°æ®ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
        
        # è½¬æ¢æ—¶é—´æˆ³
        if 'timestamp' in df.columns:
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            df.set_index('timestamp', inplace=True)
        
        print(f"âœ“ æ•°æ®åŠ è½½å®Œæˆ: {df.shape[0]} è¡Œ, {df.shape[1]} åˆ—")
        print(f"  æ—¶é—´èŒƒå›´: {df.index[0]} åˆ° {df.index[-1]}")
        
        return df
    
    def clean_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ¸…æ´—æ•°æ®
        
        å¤„ç†:
        1. åˆ é™¤é‡å¤è¡Œ
        2. æ’åºæ—¶é—´åºåˆ—
        3. å¤„ç†ç¼ºå¤±å€¼
        4. ç§»é™¤å¼‚å¸¸å€¼ï¼ˆå¯é€‰ï¼‰
        
        Args:
            df: åŸå§‹æ•°æ®
            
        Returns:
            æ¸…æ´—åçš„æ•°æ®
        """
        df = df.copy()
        print("\nğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—...")
        
        # 1. åˆ é™¤é‡å¤è¡Œ
        duplicates = df.duplicated().sum()
        if duplicates > 0:
            print(f"  âš ï¸ å‘ç° {duplicates} ä¸ªé‡å¤è¡Œï¼Œæ­£åœ¨åˆ é™¤...")
            df = df[~df.duplicated()]
        
        # 2. æ’åºï¼ˆæ—¶é—´åºåˆ—å¿…é¡»æŒ‰æ—¶é—´æ’åºï¼‰
        df = df.sort_index()
        
        # 3. æ£€æŸ¥ç¼ºå¤±å€¼
        missing = df.isnull().sum()
        if missing.any():
            print(f"  âš ï¸ å‘ç°ç¼ºå¤±å€¼:")
            for col, count in missing[missing > 0].items():
                print(f"    - {col}: {count} ä¸ª")
            
            # å‘å‰å¡«å……ï¼ˆæ—¶é—´åºåˆ—å¸¸ç”¨æ–¹æ³•ï¼‰
            df = df.fillna(method='ffill').fillna(method='bfill')
            print(f"  âœ“ ç¼ºå¤±å€¼å·²å¡«å……")
        
        # 4. ç§»é™¤ä»·æ ¼ä¸º0æˆ–è´Ÿæ•°çš„å¼‚å¸¸è¡Œ
        invalid_rows = (df['close'] <= 0) | (df['volume'] < 0)
        if invalid_rows.any():
            print(f"  âš ï¸ å‘ç° {invalid_rows.sum()} ä¸ªå¼‚å¸¸è¡Œï¼ˆä»·æ ¼â‰¤0æˆ–æˆäº¤é‡<0ï¼‰ï¼Œæ­£åœ¨åˆ é™¤...")
            df = df[~invalid_rows]
        
        print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {df.shape[0]} è¡Œä¿ç•™")
        
        return df
    
    def add_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        æ·»åŠ ç‰¹å¾ï¼ˆæŠ€æœ¯æŒ‡æ ‡ï¼‰
        
        Args:
            df: æ¸…æ´—åçš„æ•°æ®
            
        Returns:
            æ·»åŠ ç‰¹å¾åçš„æ•°æ®
        """
        print("\nğŸ”§ å¼€å§‹ç‰¹å¾å·¥ç¨‹...")
        
        # ä½¿ç”¨æŠ€æœ¯æŒ‡æ ‡ç±»æ·»åŠ æ‰€æœ‰æŒ‡æ ‡
        df = TechnicalIndicators.add_all_indicators(df)
        
        # åˆ é™¤åŒ…å«NaNçš„è¡Œï¼ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®—åˆæœŸä¼šæœ‰NaNï¼‰
        initial_rows = df.shape[0]
        df = df.dropna()
        dropped_rows = initial_rows - df.shape[0]
        
        if dropped_rows > 0:
            print(f"  â„¹ï¸ åˆ é™¤äº† {dropped_rows} è¡Œï¼ˆæŠ€æœ¯æŒ‡æ ‡è®¡ç®—åˆæœŸçš„NaNï¼‰")
        
        print(f"âœ… ç‰¹å¾å·¥ç¨‹å®Œæˆ: å½“å‰å…± {df.shape[1]} ä¸ªç‰¹å¾")
        
        return df
    
    def select_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """
        é€‰æ‹©ç”¨äºè®­ç»ƒçš„ç‰¹å¾
        
        Args:
            df: åŒ…å«æ‰€æœ‰ç‰¹å¾çš„æ•°æ®
            
        Returns:
            ä»…åŒ…å«é€‰å®šç‰¹å¾çš„æ•°æ®
        """
        # åŸºç¡€OHLCVç‰¹å¾
        base_features = ['open', 'high', 'low', 'close', 'volume']
        
        # ä»é…ç½®ä¸­è·å–æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
        if self.config.USE_TECHNICAL_INDICATORS:
            # ä½¿ç”¨é…ç½®ä¸­æŒ‡å®šçš„æŒ‡æ ‡
            selected_features = base_features + self.config.TECHNICAL_INDICATORS
        else:
            selected_features = base_features
        
        # æ£€æŸ¥å“ªäº›ç‰¹å¾å®é™…å­˜åœ¨
        available_features = [f for f in selected_features if f in df.columns]
        missing_features = [f for f in selected_features if f not in df.columns]
        
        if missing_features:
            print(f"  âš ï¸ ä»¥ä¸‹ç‰¹å¾ä¸å­˜åœ¨ï¼Œå°†è¢«è·³è¿‡: {missing_features}")
        
        print(f"  âœ“ é€‰æ‹©äº† {len(available_features)} ä¸ªç‰¹å¾: {available_features}")
        
        self.feature_columns = available_features
        return df[available_features]
    
    def normalize_data(self, df: pd.DataFrame, fit: bool = True) -> pd.DataFrame:
        """
        å½’ä¸€åŒ–æ•°æ®
        
        Args:
            df: è¦å½’ä¸€åŒ–çš„æ•°æ®
            fit: æ˜¯å¦æ‹Ÿåˆscalerï¼ˆè®­ç»ƒé›†ç”¨Trueï¼Œæµ‹è¯•é›†ç”¨Falseï¼‰
            
        Returns:
            å½’ä¸€åŒ–åçš„æ•°æ®
        """
        if fit:
            print("\nğŸ“ å¼€å§‹æ•°æ®å½’ä¸€åŒ–...")
            
            # åˆ›å»ºscaler
            if self.config.SCALER_TYPE == 'MinMaxScaler':
                self.scaler = MinMaxScaler(feature_range=self.config.FEATURE_RANGE)
            elif self.config.SCALER_TYPE == 'StandardScaler':
                self.scaler = StandardScaler()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„scalerç±»å‹: {self.config.SCALER_TYPE}")
            
            # æ‹Ÿåˆå¹¶è½¬æ¢
            scaled_data = self.scaler.fit_transform(df)
            print(f"  âœ“ ä½¿ç”¨ {self.config.SCALER_TYPE} å½’ä¸€åŒ–")
            print(f"  âœ“ æ•°æ®èŒƒå›´: {self.config.FEATURE_RANGE if self.config.SCALER_TYPE == 'MinMaxScaler' else 'æ ‡å‡†åŒ–'}")
        else:
            if self.scaler is None:
                raise ValueError("âŒ Scalerå°šæœªæ‹Ÿåˆï¼Œè¯·å…ˆåœ¨è®­ç»ƒé›†ä¸Šè°ƒç”¨fit=True")
            
            # ä»…è½¬æ¢
            scaled_data = self.scaler.transform(df)
        
        # è½¬æ¢å›DataFrameï¼ˆä¿æŒåˆ—åï¼‰
        scaled_df = pd.DataFrame(scaled_data, columns=df.columns, index=df.index)
        
        return scaled_df
    
    def create_sequences(self, data: np.ndarray, 
                        time_steps: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        åˆ›å»ºæ—¶é—´åºåˆ—çª—å£ï¼ˆæ»‘åŠ¨çª—å£ï¼‰
        
        LSTMéœ€è¦3Dè¾“å…¥: (æ ·æœ¬æ•°, æ—¶é—´æ­¥é•¿, ç‰¹å¾æ•°)
        
        ç¤ºä¾‹:
        å¦‚æœtime_steps=3, æ•°æ®=[1,2,3,4,5]
        åˆ™åˆ›å»º:
        X = [[1,2,3], [2,3,4]], y = [4, 5]
        
        Args:
            data: å½’ä¸€åŒ–åçš„æ•°æ®ï¼ˆ2D numpy arrayï¼‰
            time_steps: æ—¶é—´çª—å£å¤§å°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®
            
        Returns:
            (X, y): Xæ˜¯3Dæ•°ç»„ï¼Œyæ˜¯ç›®æ ‡å€¼
        """
        time_steps = time_steps or self.config.TIME_STEPS
        
        X, y = [], []
        
        for i in range(time_steps, len(data)):
            # X: è¿‡å»time_stepsä¸ªæ—¶é—´ç‚¹çš„æ‰€æœ‰ç‰¹å¾
            X.append(data[i - time_steps:i])
            # y: ä¸‹ä¸€ä¸ªæ—¶é—´ç‚¹çš„æ”¶ç›˜ä»·ï¼ˆå‡è®¾æ”¶ç›˜ä»·æ˜¯ç¬¬4åˆ—ï¼Œç´¢å¼•3ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œå‡è®¾ç‰¹å¾é¡ºåºä¸º [open, high, low, close, ...]
            y.append(data[i, 3])  # ç´¢å¼•3æ˜¯close
        
        X = np.array(X)
        y = np.array(y)
        
        print(f"  âœ“ åˆ›å»ºåºåˆ—: X shape = {X.shape}, y shape = {y.shape}")
        print(f"    - æ ·æœ¬æ•°: {X.shape[0]}")
        print(f"    - æ—¶é—´æ­¥é•¿: {X.shape[1]}")
        print(f"    - ç‰¹å¾æ•°: {X.shape[2]}")
        
        return X, y
    
    def split_data(self, X: np.ndarray, y: np.ndarray) -> Tuple[np.ndarray, ...]:
        """
        åˆ’åˆ†è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
        
        æ³¨æ„ï¼šæ—¶é—´åºåˆ—æ•°æ®ä¸èƒ½éšæœºæ‰“ä¹±ï¼Œå¿…é¡»æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†
        
        Args:
            X: ç‰¹å¾æ•°æ®
            y: ç›®æ ‡æ•°æ®
            
        Returns:
            (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        total_samples = len(X)
        
        # è®¡ç®—åˆ†å‰²ç‚¹
        train_size = int(total_samples * self.config.TRAIN_RATIO)
        val_size = int(total_samples * self.config.VAL_RATIO)
        
        # åˆ’åˆ†æ•°æ®
        X_train = X[:train_size]
        y_train = y[:train_size]
        
        X_val = X[train_size:train_size + val_size]
        y_val = y[train_size:train_size + val_size]
        
        X_test = X[train_size + val_size:]
        y_test = y[train_size + val_size:]
        
        print(f"\nğŸ“Š æ•°æ®é›†åˆ’åˆ†:")
        print(f"  è®­ç»ƒé›†: {X_train.shape[0]} æ ·æœ¬ ({self.config.TRAIN_RATIO*100:.0f}%)")
        print(f"  éªŒè¯é›†: {X_val.shape[0]} æ ·æœ¬ ({self.config.VAL_RATIO*100:.0f}%)")
        print(f"  æµ‹è¯•é›†: {X_test.shape[0]} æ ·æœ¬ ({self.config.TEST_RATIO*100:.0f}%)")
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def save_scaler(self, path: Optional[str] = None):
        """ä¿å­˜scalerä»¥ä¾›é¢„æµ‹æ—¶ä½¿ç”¨"""
        path = path or PathConfig.SCALER_PATH
        
        if self.scaler is None:
            raise ValueError("âŒ Scalerå°šæœªåˆå§‹åŒ–")
        
        os.makedirs(os.path.dirname(path), exist_ok=True)
        joblib.dump(self.scaler, path)
        print(f"âœ“ Scalerå·²ä¿å­˜: {path}")
    
    def load_scaler(self, path: Optional[str] = None):
        """åŠ è½½å·²ä¿å­˜çš„scaler"""
        path = path or PathConfig.SCALER_PATH
        
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ Scaleræ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        self.scaler = joblib.load(path)
        print(f"âœ“ Scalerå·²åŠ è½½: {path}")
    
    def process_all(self, file_path: Optional[str] = None, 
                   save_processed: bool = True) -> Tuple[np.ndarray, ...]:
        """
        å®Œæ•´çš„æ•°æ®å¤„ç†æµç¨‹
        
        è¿™æ˜¯ä¸€ä¸ªä¾¿æ·æ–¹æ³•ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œæ‰€æœ‰å¤„ç†æ­¥éª¤
        
        Args:
            file_path: åŸå§‹æ•°æ®æ–‡ä»¶è·¯å¾„
            save_processed: æ˜¯å¦ä¿å­˜å¤„ç†åçš„æ•°æ®
            
        Returns:
            (X_train, X_val, X_test, y_train, y_val, y_test)
        """
        print("="*60)
        print("ğŸš€ å¼€å§‹å®Œæ•´æ•°æ®å¤„ç†æµç¨‹")
        print("="*60)
        
        # 1. åŠ è½½æ•°æ®
        df = self.load_raw_data(file_path)
        
        # 2. æ¸…æ´—æ•°æ®
        df = self.clean_data(df)
        
        # 3. æ·»åŠ ç‰¹å¾
        df = self.add_features(df)
        
        # 4. é€‰æ‹©ç‰¹å¾
        df = self.select_features(df)
        
        # 5. å½’ä¸€åŒ–æ•°æ®
        df_normalized = self.normalize_data(df, fit=True)
        
        # 6. åˆ›å»ºåºåˆ—
        print("\nğŸ”„ åˆ›å»ºæ—¶é—´åºåˆ—çª—å£...")
        X, y = self.create_sequences(df_normalized.values)
        
        # 7. åˆ’åˆ†æ•°æ®é›†
        X_train, X_val, X_test, y_train, y_val, y_test = self.split_data(X, y)
        
        # 8. ä¿å­˜scaler
        self.save_scaler()
        
        # 9. ä¿å­˜å¤„ç†åçš„æ•°æ®ï¼ˆå¯é€‰ï¼‰
        if save_processed:
            self._save_processed_data(df_normalized)
        
        print("\n" + "="*60)
        print("âœ… æ•°æ®å¤„ç†æµç¨‹å®Œæˆ!")
        print("="*60)
        
        return X_train, X_val, X_test, y_train, y_val, y_test
    
    def _save_processed_data(self, df: pd.DataFrame):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        os.makedirs(self.config.LSTM_DATA_DIR, exist_ok=True)
        save_path = self.config.PROCESSED_DATA_FILE
        df.to_csv(save_path)
        print(f"âœ“ å¤„ç†åçš„æ•°æ®å·²ä¿å­˜: {save_path}")


def test_processor():
    """æµ‹è¯•æ•°æ®å¤„ç†å™¨"""
    print("LSTMæ•°æ®å¤„ç†å™¨æµ‹è¯•\n")
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    dates = pd.date_range('2023-01-01', periods=1000, freq='H')
    df = pd.DataFrame({
        'timestamp': dates,
        'open': 100 + np.random.randn(1000).cumsum(),
        'high': 102 + np.random.randn(1000).cumsum(),
        'low': 98 + np.random.randn(1000).cumsum(),
        'close': 100 + np.random.randn(1000).cumsum(),
        'volume': np.random.randint(1000, 10000, 1000)
    })
    
    # ä¿å­˜ç¤ºä¾‹æ•°æ®
    os.makedirs('data', exist_ok=True)
    df.to_csv('data/test_data.csv', index=False)
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = LSTMDataProcessor()
    
    # å¤„ç†æ•°æ®
    try:
        X_train, X_val, X_test, y_train, y_val, y_test = processor.process_all('data/test_data.csv')
        
        print(f"\næœ€ç»ˆæ•°æ®å½¢çŠ¶:")
        print(f"X_train: {X_train.shape}, y_train: {y_train.shape}")
        print(f"X_val: {X_val.shape}, y_val: {y_val.shape}")
        print(f"X_test: {X_test.shape}, y_test: {y_test.shape}")
        
        print("\nâœ… æµ‹è¯•æˆåŠŸ!")
        
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    test_processor()
