"""
LSTM æ¨¡å‹æ„å»ºæ¨¡å—
LSTM Model Builder Module

æ„å»ºå„ç§LSTMæ¶æ„çš„æ¨¡å‹
Build LSTM models with various architectures

ä½œè€…: qinshihuang166
"""

import os
import sys
import numpy as np
from typing import Tuple, List, Optional

# TensorFlow imports
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential, Model
from tensorflow.keras.layers import (
    LSTM, Bidirectional, Dense, Dropout, 
    BatchNormalization, Input, Attention,
    Layer, LayerNormalization
)
from tensorflow.keras.regularizers import l1, l2, l1_l2
from tensorflow.keras.optimizers import Adam, RMSprop, SGD
from tensorflow.keras.callbacks import (
    EarlyStopping, ModelCheckpoint, 
    ReduceLROnPlateau, TensorBoard, Callback
)

# å¯¼å…¥é…ç½®
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
from config_lstm import ModelConfig, TrainingConfig, PathConfig


class LSTMModelBuilder:
    """
    LSTMæ¨¡å‹æ„å»ºå™¨
    
    æ”¯æŒå¤šç§æ¶æ„:
    - æ ‡å‡†LSTM
    - åŒå‘LSTM (BiLSTM)
    - å¤šå±‚å †å LSTM
    - å¸¦Attentionæœºåˆ¶çš„LSTM
    """
    
    def __init__(self, config: Optional[ModelConfig] = None):
        """
        åˆå§‹åŒ–æ¨¡å‹æ„å»ºå™¨
        
        Args:
            config: æ¨¡å‹é…ç½®å¯¹è±¡
        """
        self.config = config or ModelConfig()
        self.model = None
        
    def build_model(self, input_shape: Tuple[int, int]) -> keras.Model:
        """
        æ ¹æ®é…ç½®æ„å»ºLSTMæ¨¡å‹
        
        Args:
            input_shape: è¾“å…¥å½¢çŠ¶ (time_steps, features)
            
        Returns:
            ç¼–è¯‘å¥½çš„Kerasæ¨¡å‹
        """
        print(f"\nğŸ—ï¸ å¼€å§‹æ„å»ºLSTMæ¨¡å‹...")
        print(f"  è¾“å…¥å½¢çŠ¶: {input_shape}")
        print(f"  æ¨¡å‹ç±»å‹: {self.config.MODEL_TYPE}")
        
        if self.config.MODEL_TYPE in ['LSTM', 'BiLSTM']:
            self.model = self._build_stacked_lstm(input_shape)
        elif self.config.MODEL_TYPE in ['GRU', 'BiGRU']:
            self.model = self._build_stacked_gru(input_shape)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.config.MODEL_TYPE}")
        
        # ç¼–è¯‘æ¨¡å‹
        self._compile_model()
        
        # æ‰“å°æ¨¡å‹æ‘˜è¦
        print("\nğŸ“‹ æ¨¡å‹æ¶æ„æ‘˜è¦:")
        self.model.summary()
        
        # è®¡ç®—å‚æ•°æ•°é‡
        total_params = self.model.count_params()
        print(f"\nğŸ“Š æ€»å‚æ•°æ•°é‡: {total_params:,}")
        
        return self.model
    
    def _build_stacked_lstm(self, input_shape: Tuple[int, int]) -> keras.Model:
        """
        æ„å»ºå †å LSTMæ¨¡å‹
        
        æ¶æ„:
        Input â†’ [LSTM â†’ BatchNorm â†’ Dropout] Ã— N â†’ Dense â†’ Output
        
        Args:
            input_shape: (time_steps, features)
            
        Returns:
            Kerasæ¨¡å‹
        """
        model = Sequential(name='Stacked_LSTM')
        
        # æ˜¯å¦ä½¿ç”¨åŒå‘LSTM
        use_bidirectional = self.config.MODEL_TYPE == 'BiLSTM'
        
        # è·å–æ­£åˆ™åŒ–å™¨
        regularizer = self._get_regularizer()
        
        # ç¬¬ä¸€å±‚LSTMï¼ˆéœ€è¦æŒ‡å®šinput_shapeï¼‰
        lstm_units = self.config.LSTM_UNITS
        
        for i, units in enumerate(lstm_units):
            # åˆ¤æ–­æ˜¯å¦æ˜¯æœ€åä¸€å±‚LSTM
            return_sequences = (i < len(lstm_units) - 1)
            
            # åˆ›å»ºLSTMå±‚
            lstm_layer = LSTM(
                units=units,
                return_sequences=return_sequences,
                dropout=self.config.DROPOUT_RATE,
                recurrent_dropout=self.config.RECURRENT_DROPOUT,
                kernel_regularizer=regularizer,
                name=f'lstm_{i+1}'
            )
            
            # æ˜¯å¦ä½¿ç”¨åŒå‘
            if use_bidirectional:
                lstm_layer = Bidirectional(lstm_layer, name=f'bi_lstm_{i+1}')
            
            # ç¬¬ä¸€å±‚éœ€è¦æŒ‡å®šè¾“å…¥å½¢çŠ¶
            if i == 0:
                model.add(Input(shape=input_shape, name='input'))
                model.add(lstm_layer)
            else:
                model.add(lstm_layer)
            
            # BatchNormalization
            if self.config.USE_BATCH_NORMALIZATION:
                model.add(BatchNormalization(name=f'batch_norm_{i+1}'))
            
            # Dropoutï¼ˆé¢å¤–çš„Dropoutå±‚ï¼‰
            if self.config.DROPOUT_RATE > 0 and return_sequences:
                model.add(Dropout(self.config.DROPOUT_RATE, name=f'dropout_{i+1}'))
        
        # Denseå±‚
        for i, units in enumerate(self.config.DENSE_UNITS):
            model.add(Dense(
                units=units,
                activation=self.config.DENSE_ACTIVATION,
                kernel_regularizer=regularizer,
                name=f'dense_{i+1}'
            ))
            
            # BatchNormalization
            if self.config.USE_BATCH_NORMALIZATION:
                model.add(BatchNormalization(name=f'dense_batch_norm_{i+1}'))
            
            # Dropout
            if self.config.DROPOUT_RATE > 0:
                model.add(Dropout(self.config.DROPOUT_RATE, name=f'dense_dropout_{i+1}'))
        
        # è¾“å‡ºå±‚
        model.add(Dense(
            units=1,
            activation=self.config.OUTPUT_ACTIVATION,
            name='output'
        ))
        
        return model
    
    def _build_stacked_gru(self, input_shape: Tuple[int, int]) -> keras.Model:
        """
        æ„å»ºå †å GRUæ¨¡å‹ï¼ˆç±»ä¼¼LSTMä½†æ›´ç®€å•ï¼‰
        
        Args:
            input_shape: (time_steps, features)
            
        Returns:
            Kerasæ¨¡å‹
        """
        from tensorflow.keras.layers import GRU
        
        model = Sequential(name='Stacked_GRU')
        
        use_bidirectional = self.config.MODEL_TYPE == 'BiGRU'
        regularizer = self._get_regularizer()
        
        gru_units = self.config.LSTM_UNITS  # å¤ç”¨LSTM_UNITSé…ç½®
        
        for i, units in enumerate(gru_units):
            return_sequences = (i < len(gru_units) - 1)
            
            gru_layer = GRU(
                units=units,
                return_sequences=return_sequences,
                dropout=self.config.DROPOUT_RATE,
                recurrent_dropout=self.config.RECURRENT_DROPOUT,
                kernel_regularizer=regularizer,
                name=f'gru_{i+1}'
            )
            
            if use_bidirectional:
                gru_layer = Bidirectional(gru_layer, name=f'bi_gru_{i+1}')
            
            if i == 0:
                model.add(Input(shape=input_shape, name='input'))
                model.add(gru_layer)
            else:
                model.add(gru_layer)
            
            if self.config.USE_BATCH_NORMALIZATION:
                model.add(BatchNormalization(name=f'batch_norm_{i+1}'))
            
            if self.config.DROPOUT_RATE > 0 and return_sequences:
                model.add(Dropout(self.config.DROPOUT_RATE, name=f'dropout_{i+1}'))
        
        # Denseå±‚
        for i, units in enumerate(self.config.DENSE_UNITS):
            model.add(Dense(
                units=units,
                activation=self.config.DENSE_ACTIVATION,
                kernel_regularizer=regularizer,
                name=f'dense_{i+1}'
            ))
            
            if self.config.USE_BATCH_NORMALIZATION:
                model.add(BatchNormalization(name=f'dense_batch_norm_{i+1}'))
            
            if self.config.DROPOUT_RATE > 0:
                model.add(Dropout(self.config.DROPOUT_RATE, name=f'dense_dropout_{i+1}'))
        
        # è¾“å‡ºå±‚
        model.add(Dense(units=1, activation=self.config.OUTPUT_ACTIVATION, name='output'))
        
        return model
    
    def _get_regularizer(self):
        """è·å–æ­£åˆ™åŒ–å™¨"""
        if self.config.USE_L1_REGULARIZATION and self.config.USE_L2_REGULARIZATION:
            return l1_l2(l1=self.config.L1_LAMBDA, l2=self.config.L2_LAMBDA)
        elif self.config.USE_L1_REGULARIZATION:
            return l1(self.config.L1_LAMBDA)
        elif self.config.USE_L2_REGULARIZATION:
            return l2(self.config.L2_LAMBDA)
        else:
            return None
    
    def _compile_model(self):
        """ç¼–è¯‘æ¨¡å‹"""
        print(f"\nâš™ï¸ ç¼–è¯‘æ¨¡å‹...")
        
        # é€‰æ‹©ä¼˜åŒ–å™¨
        if self.config.OPTIMIZER == 'adam':
            optimizer = Adam(learning_rate=self.config.LEARNING_RATE)
        elif self.config.OPTIMIZER == 'rmsprop':
            optimizer = RMSprop(learning_rate=self.config.LEARNING_RATE)
        elif self.config.OPTIMIZER == 'sgd':
            optimizer = SGD(learning_rate=self.config.LEARNING_RATE)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {self.config.OPTIMIZER}")
        
        # ç¼–è¯‘
        self.model.compile(
            optimizer=optimizer,
            loss=self.config.LOSS_FUNCTION,
            metrics=self.config.METRICS
        )
        
        print(f"  âœ“ ä¼˜åŒ–å™¨: {self.config.OPTIMIZER}")
        print(f"  âœ“ å­¦ä¹ ç‡: {self.config.LEARNING_RATE}")
        print(f"  âœ“ æŸå¤±å‡½æ•°: {self.config.LOSS_FUNCTION}")
        print(f"  âœ“ è¯„ä¼°æŒ‡æ ‡: {self.config.METRICS}")
    
    def get_callbacks(self) -> List[Callback]:
        """
        è·å–è®­ç»ƒå›è°ƒå‡½æ•°
        
        Returns:
            å›è°ƒå‡½æ•°åˆ—è¡¨
        """
        callbacks = []
        train_config = TrainingConfig()
        
        # 1. EarlyStopping
        if train_config.USE_EARLY_STOPPING:
            early_stopping = EarlyStopping(
                monitor=train_config.EARLY_STOPPING_MONITOR,
                patience=train_config.EARLY_STOPPING_PATIENCE,
                min_delta=train_config.EARLY_STOPPING_MIN_DELTA,
                restore_best_weights=train_config.RESTORE_BEST_WEIGHTS,
                verbose=1
            )
            callbacks.append(early_stopping)
            print(f"  âœ“ EarlyStopping (è€å¿ƒå€¼: {train_config.EARLY_STOPPING_PATIENCE})")
        
        # 2. ReduceLROnPlateau
        if train_config.USE_REDUCE_LR:
            reduce_lr = ReduceLROnPlateau(
                monitor=train_config.REDUCE_LR_MONITOR,
                factor=train_config.REDUCE_LR_FACTOR,
                patience=train_config.REDUCE_LR_PATIENCE,
                min_lr=train_config.REDUCE_LR_MIN_LR,
                verbose=1
            )
            callbacks.append(reduce_lr)
            print(f"  âœ“ ReduceLROnPlateau (å› å­: {train_config.REDUCE_LR_FACTOR})")
        
        # 3. ModelCheckpoint
        if train_config.USE_MODEL_CHECKPOINT:
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            os.makedirs(os.path.dirname(PathConfig.CHECKPOINT_PATH), exist_ok=True)
            
            checkpoint = ModelCheckpoint(
                filepath=PathConfig.CHECKPOINT_PATH,
                monitor=train_config.CHECKPOINT_MONITOR,
                mode=train_config.CHECKPOINT_MODE,
                save_best_only=train_config.CHECKPOINT_SAVE_BEST_ONLY,
                save_weights_only=train_config.CHECKPOINT_SAVE_WEIGHTS_ONLY,
                verbose=1
            )
            callbacks.append(checkpoint)
            print(f"  âœ“ ModelCheckpoint (ä¿å­˜è·¯å¾„: {PathConfig.CHECKPOINT_PATH})")
        
        # 4. TensorBoard (å¯é€‰)
        if train_config.USE_TENSORBOARD:
            log_dir = os.path.join(PathConfig.LOGS_DIR, 'tensorboard')
            os.makedirs(log_dir, exist_ok=True)
            
            tensorboard = TensorBoard(
                log_dir=log_dir,
                histogram_freq=1,
                write_graph=True
            )
            callbacks.append(tensorboard)
            print(f"  âœ“ TensorBoard (æ—¥å¿—ç›®å½•: {log_dir})")
        
        # 5. è‡ªå®šä¹‰è¿›åº¦å›è°ƒ
        progress_callback = TrainingProgressCallback()
        callbacks.append(progress_callback)
        print(f"  âœ“ TrainingProgressCallback (è®­ç»ƒè¿›åº¦æ˜¾ç¤º)")
        
        return callbacks
    
    def save_model(self, path: Optional[str] = None):
        """ä¿å­˜æ¨¡å‹"""
        if self.model is None:
            raise ValueError("âŒ æ¨¡å‹å°šæœªæ„å»º")
        
        path = path or PathConfig.MODEL_PATH
        os.makedirs(os.path.dirname(path), exist_ok=True)
        
        self.model.save(path)
        print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {path}")
    
    def load_model(self, path: Optional[str] = None) -> keras.Model:
        """åŠ è½½æ¨¡å‹"""
        path = path or PathConfig.MODEL_PATH
        
        if not os.path.exists(path):
            raise FileNotFoundError(f"âŒ æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {path}")
        
        self.model = keras.models.load_model(path)
        print(f"âœ… æ¨¡å‹å·²åŠ è½½: {path}")
        
        return self.model


class TrainingProgressCallback(Callback):
    """è‡ªå®šä¹‰è®­ç»ƒè¿›åº¦å›è°ƒ"""
    
    def __init__(self):
        super().__init__()
        self.epoch_start_time = None
    
    def on_train_begin(self, logs=None):
        print("\n" + "="*60)
        print("ğŸš€ å¼€å§‹è®­ç»ƒæ¨¡å‹...")
        print("="*60)
    
    def on_epoch_begin(self, epoch, logs=None):
        import time
        self.epoch_start_time = time.time()
    
    def on_epoch_end(self, epoch, logs=None):
        import time
        epoch_time = time.time() - self.epoch_start_time
        
        # è·å–æŒ‡æ ‡
        loss = logs.get('loss', 0)
        val_loss = logs.get('val_loss', 0)
        mae = logs.get('mae', 0)
        val_mae = logs.get('val_mae', 0)
        
        # æ‰“å°è¿›åº¦
        print(f"\nğŸ“Š Epoch {epoch + 1} å®Œæˆ (ç”¨æ—¶: {epoch_time:.2f}ç§’)")
        print(f"  è®­ç»ƒé›† - Loss: {loss:.6f}, MAE: {mae:.6f}")
        print(f"  éªŒè¯é›† - Loss: {val_loss:.6f}, MAE: {val_mae:.6f}")
        
        # å­¦ä¹ ç‡
        lr = self.model.optimizer.learning_rate
        if hasattr(lr, 'numpy'):
            lr_value = lr.numpy()
            print(f"  å½“å‰å­¦ä¹ ç‡: {lr_value:.2e}")
    
    def on_train_end(self, logs=None):
        print("\n" + "="*60)
        print("âœ… è®­ç»ƒå®Œæˆ!")
        print("="*60)


def setup_gpu():
    """é…ç½®GPU"""
    train_config = TrainingConfig()
    
    if not train_config.USE_GPU:
        # ç¦ç”¨GPU
        tf.config.set_visible_devices([], 'GPU')
        print("ğŸ’» ä½¿ç”¨ CPU è®­ç»ƒ")
        return False
    
    # æ£€æŸ¥GPUæ˜¯å¦å¯ç”¨
    gpus = tf.config.list_physical_devices('GPU')
    
    if len(gpus) == 0:
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨CPUè®­ç»ƒ")
        return False
    
    print(f"ğŸ® æ£€æµ‹åˆ° {len(gpus)} ä¸ªGPU: {gpus}")
    
    # é…ç½®GPUå†…å­˜å¢é•¿
    if train_config.GPU_MEMORY_GROWTH:
        try:
            for gpu in gpus:
                tf.config.experimental.set_memory_growth(gpu, True)
            print("  âœ“ GPUå†…å­˜åŠ¨æ€å¢é•¿å·²å¯ç”¨")
        except RuntimeError as e:
            print(f"  âš ï¸ è®¾ç½®GPUå†…å­˜å¢é•¿å¤±è´¥: {e}")
    
    # æ··åˆç²¾åº¦è®­ç»ƒ
    if train_config.MIXED_PRECISION:
        try:
            from tensorflow.keras import mixed_precision
            policy = mixed_precision.Policy('mixed_float16')
            mixed_precision.set_global_policy(policy)
            print("  âœ“ æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨ (FP16)")
        except Exception as e:
            print(f"  âš ï¸ å¯ç”¨æ··åˆç²¾åº¦å¤±è´¥: {e}")
    
    return True


def test_model_builder():
    """æµ‹è¯•æ¨¡å‹æ„å»ºå™¨"""
    print("LSTMæ¨¡å‹æ„å»ºå™¨æµ‹è¯•\n")
    
    # é…ç½®GPU
    has_gpu = setup_gpu()
    
    # åˆ›å»ºæ¨¡å‹æ„å»ºå™¨
    builder = LSTMModelBuilder()
    
    # æ„å»ºæ¨¡å‹
    input_shape = (60, 20)  # (time_steps, features)
    model = builder.build_model(input_shape)
    
    # åˆ›å»ºè™šæ‹Ÿæ•°æ®æµ‹è¯•
    X_dummy = np.random.randn(100, 60, 20).astype(np.float32)
    y_dummy = np.random.randn(100, 1).astype(np.float32)
    
    print("\nğŸ§ª æµ‹è¯•å‰å‘ä¼ æ’­...")
    predictions = model.predict(X_dummy[:10], verbose=0)
    print(f"  âœ“ é¢„æµ‹å½¢çŠ¶: {predictions.shape}")
    print(f"  âœ“ é¢„æµ‹å€¼ç¤ºä¾‹: {predictions[:3].flatten()}")
    
    print("\nâœ… æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    test_model_builder()
